import pandas as pd
import random
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
import numpy as np

#SNE classification
def categorize_SNE(actual_value):
    if actual_value < 40:
        return "Small"
    elif 40 <= actual_value < 90:
        return "Medium"
    else:
        return "High"

#Excel input file
file_path = '...' #Pathway to Excel input file
sheet_name = 'Calculated_properties'
data = pd.read_excel(file_path, sheet_name=sheet_name)
target_variable = 'ABS SNE Average'
features = ['ABS EPR parameter ratio', 'Nucleophilicity index','Adiabatic IP (eV)', 'Negative Fukui Index', 'logP']

#Loop over no of iterations
num_iterations = 1000

#Confusion matrix
labels = ["Small", "Medium", "High"]
agg_cm = np.zeros((len(labels), len(labels)))

#Feature importances
feature_importances_list = []

for iteration in range(num_iterations):
    #Data splitting into test, validation and training set
    set_molecules = ['2M5F', 'I', '4F', '5F', '6F', '7F', '4OH', '5OH', '6OH', '7OH', '4COOH',
                     '5COOH', '6COOH', '7COOH', '4M', '5M', '6M', '7M', '4OM', '5OM', '6OM', '7OM', '4N', '5N', '6N', '7N']
    random.shuffle(set_molecules)
    letter_dict = {}
    train_molecules = []
    test_molecules = []


    # Molecules with the same substituent were not repeated in both the validation and test sets
    def check_letter(molecule):
        if len(molecule) >= 2:
            letter = molecule[1:]
            if letter not in letter_dict:
                letter_dict[letter] = True
                return True
        return False

    while len(test_molecules) < 7 and set_molecules:
        molecule = random.choice(set_molecules)
        if check_letter(molecule):
            test_molecules.append(molecule)
            set_molecules.remove(molecule)

    for molecule in set_molecules:
        if molecule not in test_molecules:
            train_molecules.append(molecule)

    train_data = data[data['Molecule'].isin(train_molecules)]
    test_data = data[data['Molecule'].isin(test_molecules)]


    X_train = train_data[features]
    X_test = test_data[features]
    y_train = train_data[target_variable].apply(categorize_SNE)
    y_test = test_data[target_variable].apply(categorize_SNE)
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_scaled, y_train)
    y_pred_test = model.predict(X_test_scaled)

    #Confusion matrix for current itteration
    cm = confusion_matrix(y_test, y_pred_test, labels=labels)
    agg_cm += cm

    #Collection of feature importances
    iteration_importances = np.mean(np.abs(model.coef_), axis=0)
    feature_importances_list.append(iteration_importances)


feature_importances_array = np.array(feature_importances_list)

#Average importances
feature_importances_mean = np.mean(feature_importances_array, axis=0)
feature_importances_std = np.std(feature_importances_array, axis=0)

#Average the confusion matrix over the number of iterations
agg_cm /= num_iterations

#Normalize the confusion matrix
normalized_cm = agg_cm / agg_cm.sum(axis=1)[:, np.newaxis]

#Confusion matrix plot
plt.figure(figsize=(8, 6))
plt.imshow(normalized_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Normalized Confusion Matrix over {} Iterations'.format(num_iterations))
plt.colorbar()
tick_marks = np.arange(len(labels))
plt.xticks(tick_marks, labels, rotation=45)
plt.yticks(tick_marks, labels)
thresh = normalized_cm.max() / 2.
for i in range(len(labels)):
    for j in range(len(labels)):
        plt.text(j, i, format(normalized_cm[i, j], '.2f'), ha='center', va='center', color='white' if normalized_cm[i, j] > thresh else 'black')

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42
plt.savefig('Log_reg_confusion.pdf', format='pdf', bbox_inches='tight')
plt.show()

# Plot feature importances
plt.figure(figsize=(8, 4))
plt.barh(features, feature_importances_mean, xerr=feature_importances_std, capsize=5)
plt.xlabel('Average Coefficient Value (Absolute)')
plt.title('Feature Importance from Logistic Regression')
plt.subplots_adjust(left=0.25, right=0.95, top=0.9, bottom=0.15)
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42

plt.savefig('Log_reg_imp.pdf', format='pdf', bbox_inches='tight')
plt.show()
